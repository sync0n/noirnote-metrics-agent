======== install-agent.sh ========
#!/bin/bash

set -e # Exit immediately if a command exits with a non-zero status.

echo "--- NoirNote Agent Installer (Production v15 - FULL DATA) ---"

# --- Global Variables ---
AGENT_USER="noirnote-agent"
AGENT_DIR="/opt/noirnote-agent"
CONFIG_DIR="/etc/noirnote"
STATE_DIR="/var/lib/noirnote-agent"
AGENT_SERVICE_FILE="/etc/systemd/system/noirnote-agent.service"
AGENT_SCRIPT_PATH="${AGENT_DIR}/noirnote_agent.py"
KEY_FILE_PATH="${CONFIG_DIR}/agent-key.json"
CONFIG_FILE_PATH="${CONFIG_DIR}/agent.conf"
USER_INTEGRATIONS_CONFIG_PATH="/etc/noirnote/integrations.conf"

# Fluent Bit configuration paths
FLUENTBIT_CONF_DIR="/etc/fluent-bit/conf.d"
NOIRNOTE_FLUENTBIT_CONF="${FLUENTBIT_CONF_DIR}/noirnote.conf"
NOIRNOTE_CUSTOM_FLUENTBIT_CONF="${FLUENTBIT_CONF_DIR}/noirnote-custom.conf"
NOIRNOTE_PARSERS_CONF="/etc/fluent-bit/noirnote-parsers.conf"
SUDOERS_FILE="/etc/sudoers.d/99-noirnote-agent"

# The URLs for the cloud functions
CLAIM_URL="https://europe-west3-noirnote.cloudfunctions.net/claimAgentToken"
INGEST_URL="https://chronos.noirnote.it/ingest"

# Variable to hold the detected OS family
OS_FAMILY=""

# --- Helper and OS-Specific Functions ---

function check_root() {
    if [ "$EUID" -ne 0 ]; then
      echo "Error: This installer must be run with sudo or as root."
      exit 1
    fi
}

function cleanup_first() {
    echo "--> [Pre-flight] Performing cleanup for re-installation..."
    systemctl stop noirnote-agent.service >/dev/null 2>&1 || true
    systemctl stop fluent-bit.service >/dev/null 2>&1 || true
    echo "    - Stopped existing services (if any)."
    rm -f "$NOIRNOTE_CUSTOM_FLUENTBIT_CONF" "$SUDOERS_FILE"
    echo "    - Removed old custom configs and sudoers rule to prevent duplicates."
}

function detect_os() {
    echo "--> [1/9] Detecting operating system..."
    if [ -f /etc/os-release ]; then
        . /etc/os-release
        if [[ "$ID" == "ubuntu" || "$ID" == "debian" || "$ID_LIKE" == "debian" ]]; then
            OS_FAMILY="debian"
        elif [[ "$ID" == "centos" || "$ID" == "rhel" || "$ID" == "fedora" || "$ID_LIKE" == "rhel fedora" ]]; then
            OS_FAMILY="rhel"
        else
            echo "Error: Unsupported Linux distribution: $ID"
            exit 1
        fi
        echo "    - Detected OS Family: $OS_FAMILY"
    else
        echo "Error: Cannot detect operating system. /etc/os-release not found."
        exit 1
    fi
}

function install_dependencies() {
    echo "--> [2/9] Installing dependencies for $OS_FAMILY..."
    case "$OS_FAMILY" in
        "debian")
            echo "    - Updating package lists..."
            apt-get update -y > /dev/null

            apt-get install -y curl gpg lsb-release > /dev/null
            . /etc/os-release

            if [ "$ID" = "ubuntu" ]; then
                UBUNTU_VERSION=$(lsb_release -rs)
                case "$UBUNTU_VERSION" in
                  24.04|24.*) REPO_CODENAME="noble" ;;
                  22.04|22.*) REPO_CODENAME="jammy" ;;
                  20.04|20.*) REPO_CODENAME="focal" ;;
                  *)
                    echo "    [ERROR] Unsupported Ubuntu version: '$UBUNTU_VERSION'."
                    echo "    This agent requires Ubuntu 24.04, 22.04, or 20.04 LTS."
                    exit 1
                    ;;
                esac
                REPO_PATH="ubuntu/${REPO_CODENAME}"
            elif [ "$ID" = "debian" ]; then
                DEBIAN_VERSION=$(lsb_release -rs)
                case "$DEBIAN_VERSION" in
                  12|12.*) REPO_CODENAME="bookworm" ;;
                  11|11.*) REPO_CODENAME="bullseye" ;;
                  *)
                    echo "    [ERROR] Unsupported Debian version: '$DEBIAN_VERSION'."
                    echo "    This agent requires Debian 12 or 11."
                    exit 1
                    ;;
                esac
                REPO_PATH="debian/${REPO_CODENAME}"
            else
                echo "    - Error: Unrecognized Debian-family OS: '$ID'. Cannot set up repository."
                exit 1
            fi

            echo "    - Removing any pre-existing Fluent Bit repository files..."
            rm -f /etc/apt/sources.list.d/fluent-bit.list /etc/apt/sources.list.d/fluentbit.list
            
            echo "    - Configuring Fluent Bit repository for ${REPO_PATH}..."
            mkdir -p /etc/apt/keyrings
            curl -s https://packages.fluentbit.io/fluentbit.key > /etc/apt/keyrings/fluentbit.asc
            echo "deb [signed-by=/etc/apt/keyrings/fluentbit.asc] https://packages.fluentbit.io/${REPO_PATH} ${REPO_CODENAME} main" > /etc/apt/sources.list.d/fluentbit.list

            echo "    - Updating package lists again..."
            apt-get update -y > /dev/null
            apt-get install -y python3 python3-pip python3-venv net-tools fluent-bit > /dev/null
            ;;
        "rhel")
            PKG_MANAGER=$(command -v dnf || command -v yum)
            $PKG_MANAGER makecache > /dev/null
            tee /etc/yum.repos.d/fluent-bit.repo > /dev/null <<'YUM_REPO_EOF'
[fluent-bit]
name = Fluent Bit
baseurl = https://packages.fluentbit.io/centos/8/$basearch/
gpgcheck=1
gpgkey=https://packages.fluentbit.io/fluentbit.key
enabled=1
YUM_REPO_EOF
            $PKG_MANAGER install -y python3 python3-pip curl net-tools fluent-bit > /dev/null
            ;;
    esac
    
    pip3 install --break-system-packages psutil==5.9.8 requests==2.32.3 google-auth==2.28.2 pycryptodome==3.20.0 > /dev/null
    
    echo "    - Dependencies installed."
}

function setup_agent_user_and_dirs() {
    echo "--> [3/9] Setting up user and directories..."
    if ! id -u "$AGENT_USER" >/dev/null 2>&1; then
        useradd --system --shell /usr/sbin/nologin "$AGENT_USER"
        echo "    - Created system user '$AGENT_USER'"
    else
        echo "    - System user '$AGENT_USER' already exists."
    fi

    usermod -a -G adm,systemd-journal ${AGENT_USER}
    # Add root to the agent's group so Fluent Bit (running as root) can write to the state dir
    usermod -a -G ${AGENT_USER} root
    echo "    - Granted '$AGENT_USER' read access to system logs."
    echo "    - Added 'root' user to '${AGENT_USER}' group for Fluent Bit."
    
    mkdir -p "$AGENT_DIR" "$CONFIG_DIR" "$STATE_DIR"
    chown -R "$AGENT_USER":"$AGENT_USER" "$AGENT_DIR" "$CONFIG_DIR" "$STATE_DIR"
    chmod 750 "$AGENT_DIR" "$CONFIG_DIR"
    # State dir needs group-write permissions for Fluent Bit (as part of the group)
    chmod 770 "$STATE_DIR"
    echo "    - Set secure directory permissions (770 for state dir)."
}

function setup_sudoers() {
    echo "--> [4/9] Configuring sudoers for privileged data collection..."
    # --- NEW: Add more commands to the sudoers file for full data collection ---
    tee "$SUDOERS_FILE" > /dev/null <<'SUDOERS_EOF'
# Allow noirnote-agent to run specific commands with root privileges for data collection.
noirnote-agent ALL=(ALL) NOPASSWD: /usr/bin/ss, /usr/bin/dmesg, /usr/sbin/iptables, /usr/bin/systemctl, /usr/bin/journalctl, /usr/bin/last, /usr/bin/who
SUDOERS_EOF
    # --- END NEW ---

    chmod 440 "$SUDOERS_FILE"
    echo "    - Sudoers rule created and secured for data collection commands."
}

function configure_fluent_bit() {
    echo "--> [5/9] Configuring Fluent Bit for structured log collection..."

    tee "/etc/fluent-bit/fluent-bit.conf" > /dev/null <<'FLUENTBIT_MAIN_EOF'
[SERVICE]
    Flush           5
    Daemon          Off
    Log_Level       info
    Parsers_File    noirnote-parsers.conf
    @INCLUDE        conf.d/*.conf
FLUENTBIT_MAIN_EOF

    mkdir -p "$FLUENTBIT_CONF_DIR"

    tee "$NOIRNOTE_FLUENTBIT_CONF" > /dev/null <<'FLUENTBIT_NOIRNOTE_EOF'
[INPUT]
    Name            systemd
    Tag             noirnote.host.*
    Systemd_Filter  _SYSTEMD_UNIT
    Read_from_Head  On

[INPUT]
    Name            tail
    Tag             noirnote.nginx.error
    Path            /var/log/nginx/error.log*
    Parser          nginx_error
    Multiline.parser  docker, cri
    Read_from_Head  On

[INPUT]
    Name            tail
    Tag             noirnote.apache.error
    Path            /var/log/apache2/error.log*
    Multiline.parser  docker, cri
    Read_from_Head  On

[INPUT]
    Name            tail
    Tag             noirnote.httpd.error
    Path            /var/log/httpd/error_log*
    Multiline.parser  docker, cri
    Read_from_Head  On

[INPUT]
    Name            tail
    Tag             noirnote.postgres.log
    Path            /var/log/postgresql/*.log
    Parser          postgres_log
    Read_from_Head  On

[OUTPUT]
    Name            file
    Match           noirnote.*
    Path            /var/lib/noirnote-agent/structured_logs.json
    Format          json_lines
FLUENTBIT_NOIRNOTE_EOF

    tee "$NOIRNOTE_PARSERS_CONF" > /dev/null <<'FLUENTBIT_PARSERS_EOF'
[PARSER]
    Name   nginx_error
    Format regex
    Regex  ^(?<time>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<level>\w+)\] (?<pid>\d+#\d+): (?<tid>\*\d+)?(?<message>.*)$

[PARSER]
    Name   postgres_log
    Format regex
    Regex  ^(?<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d+ \w+)\s\[(?<pid>\d+)\]\s(?<message>.*)$
FLUENTBIT_PARSERS_EOF

    echo "    - Fluent Bit configured for multi-OS log paths."
}

function configure_custom_integrations() {
    echo "--> [6/9] Configuring Custom Integrations..."
    if [ ! -f "$USER_INTEGRATIONS_CONFIG_PATH" ]; then
        echo "    - No custom integrations file found at '$USER_INTEGRATIONS_CONFIG_PATH'. Skipping."
        touch "$NOIRNOTE_CUSTOM_FLUENTBIT_CONF"
        return
    fi
    
    echo "    - Found custom integrations file. Generating Fluent Bit config..."
    
    awk '
        BEGIN { printing = 0 }
        /^\[logs\]$/ { printing = 1; next }
        /^\[.*\]$/ { printing = 0 }
        printing && /=/ { print }
    ' "$USER_INTEGRATIONS_CONFIG_PATH" | while IFS='=' read -r name path; do
        name=$(echo "$name" | xargs)
        path=$(echo "$path" | xargs)
        
        if [ -n "$name" ] && [ -n "$path" ]; then
            echo "    - Adding custom log: '$name' at '$path'"
            tee -a "$NOIRNOTE_CUSTOM_FLUENTBIT_CONF" > /dev/null <<EOF
[INPUT]
    Name            tail
    Tag             noirnote.custom.${name}
    Path            ${path}
    Multiline.parser  docker, cri
    Read_from_Head  On

EOF
        fi
    done

    if [ ! -s "$NOIRNOTE_CUSTOM_FLUENTBIT_CONF" ]; then
        echo "    - No valid log entries found in '$USER_INTEGRATIONS_CONFIG_PATH'."
    else
        echo "    - Custom Fluent Bit configuration created at '$NOIRNOTE_CUSTOM_FLUENTBIT_CONF'."
    fi
}

function create_agent_script() {
    echo "--> [7/9] Creating agent script at ${AGENT_SCRIPT_PATH}..."
    tee "$AGENT_SCRIPT_PATH" > /dev/null <<'AGENT_EOF'
# agent/noirnote_agent.py (Production v15 - FULL DATA)
import psutil, requests, json, time, os, traceback, re, platform, subprocess, glob
from datetime import datetime, timezone
from google.oauth2 import service_account
import google.auth.transport.requests
import base64
from Crypto.Cipher import AES
from Crypto.Random import get_random_bytes

CONFIG_FILE_PATH = "/etc/noirnote/agent.conf"
KEY_FILE_PATH = "/etc/noirnote/agent-key.json"
STATE_FILE_PATH = "/var/lib/noirnote-agent/state.json"
STRUCTURED_LOG_PATH = "/var/lib/noirnote-agent/structured_logs.json"
USER_INTEGRATIONS_CONFIG_PATH = "/etc/noirnote/integrations.conf"

last_net_io, last_disk_io, last_time = psutil.net_io_counters(), psutil.disk_io_counters(), time.time()

INTEGRATION_KNOWLEDGE_MAP = {
    "nginx": {"config_paths": ["/etc/nginx/nginx.conf"], "version_command": "nginx -v", "pid_path": "/var/run/nginx.pid"},
    "postgres": {"config_paths": ["/etc/postgresql/*/main/postgresql.conf"], "version_command": "psql --version", "pid_path": "/var/run/postgresql/*.pid"},
    "httpd": {"config_paths": ["/etc/httpd/conf/httpd.conf"], "version_command": "httpd -v", "pid_path": "/var/run/httpd/httpd.pid"},
    "apache2": {"config_paths": ["/etc/apache2/apache2.conf"], "version_command": "apache2 -v", "pid_path": "/var/run/apache2/apache2.pid"},
    "mysqld": {"config_paths": ["/etc/mysql/my.cnf", "/etc/my.cnf"], "version_command": "mysql --version", "pid_path": "/var/run/mysqld/mysqld.pid"},
    "redis-server": {"config_paths": ["/etc/redis/redis.conf"], "version_command": "redis-server --version", "pid_path": "/var/run/redis/redis-server.pid"}
}

def encrypt_payload(plaintext_bytes: bytes, key: bytes) -> (str, str):
    try:
        cipher = AES.new(key, AES.MODE_GCM)
        nonce = cipher.nonce
        ciphertext, tag = cipher.encrypt_and_digest(plaintext_bytes)
        combined_ciphertext = ciphertext + tag
        return (
            base64.b64encode(combined_ciphertext).decode('utf-8'),
            base64.b64encode(nonce).decode('utf-8')
        )
    except Exception as e:
        print(f"FATAL: Encryption failed: {e}")
        traceback.print_exc()
        raise

def load_state():
    if not os.path.exists(STATE_FILE_PATH): return {}
    try:
        with open(STATE_FILE_PATH, 'r') as f: return json.load(f)
    except (json.JSONDecodeError, IOError, PermissionError): return {}

def save_state(state):
    try:
        with open(STATE_FILE_PATH, 'w') as f: json.dump(state, f, indent=2)
    except (IOError, PermissionError): pass

def load_user_integrations():
    empty_integrations = {"logs": [], "configs": []}
    if not os.path.exists(USER_INTEGRATIONS_CONFIG_PATH): return empty_integrations
    integrations = {"logs": [], "configs": []}
    try:
        with open(USER_INTEGRATIONS_CONFIG_PATH, 'r') as f:
            current_section = None
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'): continue
                if line.startswith('[') and line.endswith(']'):
                    current_section = line[1:-1].lower()
                    continue
                if current_section and '=' in line:
                    key, value = line.split('=', 1)
                    if current_section in integrations:
                        integrations[current_section].append({"name": key.strip(), "path": value.strip()})
    except (IOError, PermissionError): return empty_integrations
    return integrations

def _run_command(command):
    try:
        # If the command is one we have sudo privileges for, use sudo.
        sudo_commands = ["ss", "dmesg", "iptables", "systemctl", "journalctl", "last", "who"]
        if any(cmd in command for cmd in sudo_commands):
            command = "sudo " + command
        return subprocess.run(command, shell=True, capture_output=True, text=True, timeout=20, check=False).stdout.strip()
    except Exception: return ""

def _read_pid_from_file(pid_path_pattern):
    try:
        pid_files = glob.glob(pid_path_pattern)
        if not pid_files: return None
        with open(pid_files[0], 'r') as f: return int(f.read().strip())
    except (IOError, PermissionError, ValueError, IndexError): return None

# --- NEW: Add the missing data collection functions ---
def get_firewall_rules():
    output = _run_command("iptables -S")
    return [line for line in output.splitlines() if line.strip() and not line.startswith('#')]

def get_systemd_services():
    output = _run_command("systemctl list-units --type=service --all --no-pager")
    services = []
    lines = output.splitlines()
    if lines and 'UNIT' in lines[0]:
        lines = lines[1:] # Skip header
    for line in lines:
        if '.service' in line:
            # Handle potential empty lines from systemctl output
            line_cleaned = line.strip().replace('●', '').strip()
            parts = re.split(r'\s+', line_cleaned, 4)
            if len(parts) >= 4:
                services.append({"unit": parts[0], "load": parts[1], "active": parts[2], "sub": parts[3], "description": parts[4] if len(parts) > 4 else ""})
    return services

def get_system_logs():
    output = _run_command("journalctl -n 100 --no-pager --priority=err..warning")
    return output.splitlines()

def get_active_connections():
    output = _run_command("ss -tuna")
    connections = []
    # Skip the header line
    for line in output.splitlines()[1:]:
        try:
            parts = line.split()
            # Netid, State, Recv-Q, Send-Q, Local Address:Port, Peer Address:Port
            if len(parts) >= 6:
                 connections.append({"protocol": parts[0], "state": parts[1], "local_address": parts[4], "peer_address": parts[5]})
        except IndexError:
            continue
    return connections

def get_recent_logins():
    return _run_command("last -n 20").splitlines()

def get_active_sessions():
    return _run_command("who").splitlines()

def get_failed_logins():
    # A more robust grep that finds the common failure messages
    return _run_command("journalctl _COMM=sshd --no-pager -n 50 | grep -E 'Failed|failure'").splitlines()
# --- END NEW ---

def get_structured_dmesg():
    output = _run_command("dmesg -T")
    pattern = re.compile(r'\[\s*([^\]]+)\]\s*(.*)')
    return [{"timestamp": m.group(1).strip(), "message": m.group(2).strip()} for line in output.splitlines() if (m := pattern.match(line))]

def get_ss_listeners():
    output = _run_command("ss -tulpn")
    listeners = []
    for line in output.splitlines()[1:]:
        try:
            parts = line.split()
            match = re.search(r'users:\(\("([^"]+)",pid=(\d+),', parts[-1])
            proc_name, pid = (match.group(1), int(match.group(2))) if match else (None, None)
            listeners.append({"protocol": parts[0], "state": parts[1], "local_address": parts[4], "process_name": proc_name, "pid": pid})
        except (IndexError, ValueError): continue
    return listeners

def get_version_info(command):
    raw = _run_command(command)
    match = re.search(r'(\d+\.\d+\.\d+)', raw)
    return {"version": match.group(1) if match else None, "raw": raw}

def collect_state_snapshot(discovered_services: list):
    # --- NEW: Add calls to the new data collection functions ---
    snapshot = {
        'dmesg_events': get_structured_dmesg(),
        'network_listeners': get_ss_listeners(),
        'active_connections': get_active_connections(),
        'firewall_rules': get_firewall_rules(),
        'system_services': get_systemd_services(),
        'system_logs': get_system_logs(),
        'recent_logins': get_recent_logins(),
        'active_sessions': get_active_sessions(),
        'failed_logins': get_failed_logins(),
        'versions': {},
        'configs': {}
    }
    # --- END NEW ---
    for service in discovered_services:
        k = INTEGRATION_KNOWLEDGE_MAP.get(service, {})
        if k.get("version_command"): snapshot['versions'][service] = get_version_info(k["version_command"])
        pid = _read_pid_from_file(k["pid_path"]) if k.get("pid_path") else None
        
        config_content = "Config path not found or not readable."
        if "config_paths" in k:
            for path_pattern in k["config_paths"]:
                for path in glob.glob(path_pattern):
                    try:
                        with open(path, 'r', errors='ignore') as f: config_content = f.read()
                        break
                    except (IOError, PermissionError, FileNotFoundError): continue
                if "not found" not in config_content: break
        snapshot['configs'][service] = {"pid": pid, "content": config_content}

    user_integrations = load_user_integrations()
    for custom_config in user_integrations.get("configs", []):
        content = f"File not found or not readable at {custom_config['path']}"
        try:
            with open(custom_config['path'], 'r', errors='ignore') as f: content = f.read()
        except (IOError, PermissionError) as e: content = f"Error reading {custom_config['path']}: {e}"
        snapshot['configs'][custom_config['name']] = {"pid": None, "content": content}

    return snapshot

def _read_new_lines_from_file(path, state):
    lines, key = [], f"log_{path}"
    if not os.path.exists(path): return lines
    try:
        inode = os.stat(path).st_ino
        log_state = state.get(key, {})
        offset = log_state.get('offset', 0) if log_state.get('inode') == inode else 0
        with open(path, 'rb') as f:
            f.seek(offset)
            lines = [line.decode('utf-8', 'ignore').strip() for line in f.readlines()]
            state[key] = {'inode': inode, 'offset': f.tell()}
    except (IOError, PermissionError, FileNotFoundError): pass
    return lines

def collect_events(state):
    return [json.loads(line) for line in _read_new_lines_from_file(STRUCTURED_LOG_PATH, state) if line]

def collect_all_metrics():
    global last_net_io, last_disk_io, last_time
    now, elapsed = time.time(), 1.0
    if now > last_time: elapsed = now - last_time
    net, disk = psutil.net_io_counters(), psutil.disk_io_counters()
    
    procs, services = [], set()
    for p in psutil.process_iter(['pid', 'name', 'username', 'cpu_percent', 'memory_percent', 'cmdline']):
        try:
            info = p.info
            procs.append(info)
            if info['name'] in INTEGRATION_KNOWLEDGE_MAP: services.add(info['name'])
        except (psutil.NoSuchProcess, psutil.AccessDenied): continue

    tcp_states = {"ESTABLISHED": 0, "TIME_WAIT": 0, "CLOSE_WAIT": 0}
    try:
        for conn in psutil.net_connections(kind='tcp'):
            if conn.status in tcp_states: tcp_states[conn.status] += 1
    except psutil.AccessDenied: pass

    # --- NEW: Calculate and add top-level cpu_percent ---
    cpu_times = psutil.cpu_times_percent()
    cpu_percent_val = 100.0 - cpu_times.idle
    # --- END NEW ---

    metrics = {
        "cpu_percent": cpu_percent_val, # --- NEW ---
        "cpu_states": cpu_times._asdict(),
        "memory_details": psutil.virtual_memory()._asdict(),
        "disk_io": {"reads_per_sec": (disk.read_count - last_disk_io.read_count) / elapsed, "writes_per_sec": (disk.write_count - last_disk_io.write_count) / elapsed},
        "disks": [{"device": p.device, "mountpoint": p.mountpoint, "percent": psutil.disk_usage(p.mountpoint).percent} for p in psutil.disk_partitions(all=False) if 'loop' not in p.device],
        "network": {"sent_per_sec": (net.bytes_sent - last_net_io.bytes_sent) / elapsed, "recv_per_sec": (net.bytes_recv - last_net_io.bytes_recv) / elapsed, "tcp_connections": tcp_states},
        "processes": sorted(procs, key=lambda p: (p.get('cpu_percent', 0) or 0), reverse=True)[:15],
    }
    last_net_io, last_disk_io, last_time = net, disk, now
    return metrics, list(services)

def main():
    print("Starting NoirNote Agent...")
    config = {k.strip(): v.strip() for line in open(CONFIG_FILE_PATH) if '=' in line for k, v in [line.strip().split('=', 1)]}
    
    chronos_key_b64 = config.get('CHRONOS_ENCRYPTION_KEY')
    if not chronos_key_b64:
        print("FATAL: CHRONOS_ENCRYPTION_KEY not found in config. Agent cannot run.")
        return
    try:
        chronos_key = base64.b64decode(chronos_key_b64)
        if len(chronos_key) != 32: raise ValueError("Decoded key is not 32 bytes.")
    except Exception as e:
        print(f"FATAL: Could not decode CHRONOS_ENCRYPTION_KEY from config: {e}"); return

    creds = service_account.IDTokenCredentials.from_service_account_file(KEY_FILE_PATH, target_audience=config['INGEST_FUNCTION_URL'])
    state = load_state()
    session = google.auth.transport.requests.Request()
    while True:
        try:
            metrics, services = collect_all_metrics()
            
            data_to_encrypt = { "metrics": metrics, "events": collect_events(state), "state": collect_state_snapshot(services) }
            plaintext_json_bytes = json.dumps(data_to_encrypt).encode('utf-8')
            
            encrypted_payload_b64, nonce_b64 = encrypt_payload(plaintext_json_bytes, chronos_key)
            
            payload = {
                "user_id": config['USER_ID'],
                "workspace_id": config.get('WORKSPACE_ID', config['USER_ID']),
                "server_id": config['SERVER_ID'],
                "encrypted_payload_b64": encrypted_payload_b64,
                "nonce_b64": nonce_b64,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }

            creds.refresh(session)
            headers = {'Authorization': f'Bearer {creds.token}', 'Content-Type': 'application/json'}
            resp = requests.post(config['INGEST_FUNCTION_URL'], json=payload, headers=headers, timeout=30)
            resp.raise_for_status()
            print(f"Successfully pushed ENCRYPTED payload. Status: {resp.status_code}")
            save_state(state)
        except Exception as e:
            print(f"ERROR: An unhandled exception occurred in the main loop: {e}")
            traceback.print_exc()
        time.sleep(int(config.get('INTERVAL_SECONDS', 60)))

if __name__ == "__main__":
    main()
AGENT_EOF
    chown "$AGENT_USER":"$AGENT_USER" "$AGENT_SCRIPT_PATH"
    chmod 750 "$AGENT_SCRIPT_PATH"
}

function configure_agent() {
    echo "--> [8/9] Configuring agent..."
    TOKEN=""
    for arg in "$@"; do
        if [[ $arg == --token=* ]]; then TOKEN="${arg#*=}"; shift; fi
    done

    if [ -f "$CONFIG_FILE_PATH" ] && [ -f "$KEY_FILE_PATH" ] && [ -z "$TOKEN" ]; then
        echo "    - Configuration files already exist. Skipping."
        return
    fi
    if [ -z "$TOKEN" ]; then
        echo "    [ERROR] --token flag is required for initial configuration."; exit 1
    fi

    echo "    - Claiming credentials..."
    RESPONSE_JSON=$(curl -s -X POST -H "Content-Type: application/json" -d "{\"token\": \"$TOKEN\"}" "$CLAIM_URL")

    if ! echo "$RESPONSE_JSON" | grep -q "private_key"; then
        echo "    [ERROR] Failed to claim agent credentials. Response: $RESPONSE_JSON"; exit 1
    fi

    SERVICE_ACCOUNT_KEY_JSON=$(echo "$RESPONSE_JSON" | python3 -c "import sys, json; print(json.dumps(json.load(sys.stdin).get('serviceAccountKey'), indent=2))")
    USER_ID=$(echo "$RESPONSE_JSON" | python3 -c "import sys, json; print(json.load(sys.stdin).get('userId', ''))")
    SERVER_ID=$(echo "$RESPONSE_JSON" | python3 -c "import sys, json; print(json.load(sys.stdin).get('serverId', ''))")
    CHRONOS_KEY=$(echo "$RESPONSE_JSON" | python3 -c "import sys, json; print(json.load(sys.stdin).get('chronosKey', ''))")
    
    if [ -z "$SERVICE_ACCOUNT_KEY_JSON" ] || [ -z "$USER_ID" ] || [ -z "$SERVER_ID" ] || [ -z "$CHRONOS_KEY" ]; then
        echo "    [ERROR] Claim response was incomplete."; exit 1
    fi
    
    echo "$SERVICE_ACCOUNT_KEY_JSON" > "$KEY_FILE_PATH"
    chown "$AGENT_USER":"$AGENT_USER" "$KEY_FILE_PATH"
    chmod 400 "$KEY_FILE_PATH"

    tee "$CONFIG_FILE_PATH" > /dev/null <<EOF
SERVER_ID=$SERVER_ID
USER_ID=$USER_ID
WORKSPACE_ID=$USER_ID
INGEST_FUNCTION_URL=$INGEST_URL
INTERVAL_SECONDS=60
CHRONOS_ENCRYPTION_KEY=$CHRONOS_KEY
EOF
    chown "$AGENT_USER":"$AGENT_USER" "$CONFIG_FILE_PATH"
    chmod 640 "$CONFIG_FILE_PATH"
    echo "    - Configuration saved for server: '$SERVER_ID'."
}

function setup_service() {
    echo "--> [9/9] Setting up and starting systemd services..."
    tee "$AGENT_SERVICE_FILE" > /dev/null <<'SERVICE_EOF'
[Unit]
Description=NoirNote Structured Data Agent
After=network-online.target fluent-bit.service
Wants=network-online.target fluent-bit.service

[Service]
Type=simple
User=noirnote-agent
Group=noirnote-agent
SupplementaryGroups=adm systemd-journal
ExecStart=/usr/bin/python3 -u /opt/noirnote-agent/noirnote_agent.py
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
SERVICE_EOF
    
    systemctl daemon-reload
    systemctl enable fluent-bit.service || echo "Warning: Could not enable fluent-bit service."
    systemctl restart fluent-bit.service || echo "Warning: Could not restart fluent-bit service."
    systemctl enable noirnote-agent.service
    systemctl restart noirnote-agent.service
    
    echo ""
    echo "--- Installation Complete! ---"
    echo "The NoirNote agent and Fluent Bit are now running."
    echo "To check agent status:      systemctl status noirnote-agent.service"
    echo "To check fluent-bit status: systemctl status fluent-bit.service"
    echo "To view live agent logs:    journalctl -u noirnote-agent.service -f"
}

# --- Main Execution ---
main() {
    check_root
    cleanup_first
    detect_os
    install_dependencies
    setup_agent_user_and_dirs
    setup_sudoers
    configure_fluent_bit
    configure_custom_integrations
    create_agent_script
    configure_agent "$@"
    setup_service
}

main "$@"

======== noirnote_agent.py ========
Of course. Here is the complete, updated noirnote_agent.py script with all the requested features for advanced state collection, formatted for readability.

code
Python
download
content_copy
expand_less

# agent/noirnote_agent.py
import psutil
import requests
import json
import time
import os
import traceback
import re
import platform
import subprocess
from datetime import datetime
from google.oauth2 import service_account
import google.auth.transport.requests

# --- Configuration ---
CONFIG_FILE_PATH = "/etc/noirnote/agent.conf"
KEY_FILE_PATH = "/etc/noirnote/agent-key.json"
STATE_FILE_PATH = "/var/lib/noirnote-agent/state.json"

# --- State for calculating network rate ---
last_net_io = psutil.net_io_counters()
last_time = time.time()

# --- State Management Functions (for event processing) ---

def load_state():
    """Loads the agent's state from a file to avoid reprocessing events."""
    if not os.path.exists(STATE_FILE_PATH):
        return {}
    try:
        with open(STATE_FILE_PATH, 'r') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError, PermissionError) as e:
        print(f"WARN: Could not load agent state from {STATE_FILE_PATH}, starting fresh. Error: {e}")
        return {}

def save_state(state):
    """Saves the agent's state to a file."""
    try:
        with open(STATE_FILE_PATH, 'w') as f:
            json.dump(state, f, indent=2)
    except (IOError, PermissionError) as e:
        print(f"ERROR: Could not save agent state to {STATE_FILE_PATH}: {e}")

# --- State Snapshot Collection (for AI Root Cause Analysis) ---

def _run_command(command):
    """A robust helper to run a shell command and return its output or an error."""
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=20,
            check=False  # Don't raise exception on non-zero exit codes
        )
        if result.returncode != 0:
            # Combine stdout and stderr for better error context
            error_output = (result.stdout.strip() + " " + result.stderr.strip()).strip()
            return f"Error executing command '{command}': [Exit Code {result.returncode}] {error_output}"
        return result.stdout.strip()
    except FileNotFoundError:
        return f"Error: Command not found for '{command}'."
    except subprocess.TimeoutExpired:
        return f"Error: Command '{command}' timed out after 20 seconds."
    except Exception as e:
        return f"An unexpected error occurred while running '{command}': {e}"

def get_dmesg_output():
    """Gets kernel ring buffer messages."""
    return _run_command("dmesg -T")

def get_netstat_output():
    """Gets network connection and listening port information."""
    return _run_command("netstat -tulnp")

def get_syslog_snippet():
    """Gets the last 200 lines from journalctl or syslog."""
    if os.path.exists('/bin/journalctl'):
        return _run_command("journalctl -n 200 --no-pager")
    elif os.path.exists('/var/log/syslog'):
        return _run_command("tail -n 200 /var/log/syslog")
    else:
        return "Neither journalctl nor /var/log/syslog found."

def get_messages_log_snippet():
    """Gets the last 200 lines from /var/log/messages (for RHEL/CentOS)."""
    if os.path.exists('/var/log/messages'):
        return _run_command("tail -n 200 /var/log/messages")
    return ""  # Return empty if file doesn't exist, not an error

def get_kern_log_snippet():
    """Gets the last 200 lines from /var/log/kern.log."""
    if os.path.exists('/var/log/kern.log'):
        return _run_command("tail -n 200 /var/log/kern.log")
    return ""

def get_auth_log_snippet():
    """Gets the last 200 lines from the auth log."""
    if os.path.exists('/var/log/auth.log'):
        return _run_command("tail -n 200 /var/log/auth.log")
    elif os.path.exists('/var/log/secure'):
        return _run_command("tail -n 200 /var/log/secure")
    return ""

def get_windows_event_logs():
    """Queries and formats the last 50 System and Security events on Windows."""
    ps_command = """
    $logs = @{
        "System" = Get-WinEvent -LogName System -MaxEvents 50 | Select-Object TimeCreated, LevelDisplayName, Message -ErrorAction SilentlyContinue;
        "Security" = Get-WinEvent -LogName Security -MaxEvents 50 | Select-Object TimeCreated, Message -ErrorAction SilentlyContinue
    }
    $logs | ConvertTo-Json -Compress -Depth 3
    """
    try:
        result = subprocess.run(
            ["powershell", "-Command", ps_command],
            capture_output=True,
            text=True,
            timeout=30,
            check=True
        )
        return json.loads(result.stdout.strip())
    except FileNotFoundError:
        return {"error": "PowerShell is not installed or not in PATH."}
    except subprocess.CalledProcessError as e:
        return {"error": f"PowerShell command failed: {e.stderr}"}
    except subprocess.TimeoutExpired:
        return {"error": "PowerShell command timed out after 30 seconds."}
    except json.JSONDecodeError:
        return {"error": "Failed to parse JSON output from PowerShell."}
    except Exception as e:
        return {"error": f"An unexpected error occurred while fetching Windows events: {e}"}

def collect_state_snapshot():
    """
    Orchestrates the collection of a comprehensive state snapshot for analysis.
    """
    print("--> Collecting state snapshot...")
    snapshot = {}
    system = platform.system()

    if system == "Linux":
        snapshot['dmesg'] = get_dmesg_output()
        snapshot['netstat'] = get_netstat_output()
        snapshot['syslog_snippet'] = get_syslog_snippet()
        snapshot['messages_log_snippet'] = get_messages_log_snippet()
        snapshot['kern_log_snippet'] = get_kern_log_snippet()
        snapshot['auth_log_snippet'] = get_auth_log_snippet()
    elif system == "Windows":
        snapshot['windows_event_logs'] = get_windows_event_logs()

    print("--> State snapshot collection complete.")
    return snapshot

# --- Event Collection ---

def _read_new_log_lines(log_path, state):
    """Generic helper to read new lines from a log file, handling log rotation."""
    new_lines = []
    if not os.path.exists(log_path):
        return new_lines
    try:
        current_inode = os.stat(log_path).st_ino
    except (FileNotFoundError, PermissionError) as e:
        print(f"WARN: Cannot stat log file {log_path}: {e}")
        return new_lines
    log_state = state.get(log_path, {})
    last_inode = log_state.get('inode')
    last_offset = log_state.get('offset', 0)
    if current_inode != last_inode:
        last_offset = 0
    try:
        with open(log_path, 'rb') as f:
            f.seek(last_offset)
            raw_lines = f.readlines()
            new_offset = f.tell()
    except (IOError, PermissionError) as e:
        print(f"WARN: Could not read from log file {log_path}: {e}")
        return new_lines
    new_lines = [line.decode('utf-8', errors='ignore').strip() for line in raw_lines]
    state[log_path] = {'inode': current_inode, 'offset': new_offset}
    return new_lines

def _create_event(event_type, summary):
    """Standardizes event creation."""
    return {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "event_type": event_type,
        "summary": summary
    }

def parse_auth_log(state):
    """Parses auth.log or secure for login events."""
    events = []
    log_file = '/var/log/auth.log' if os.path.exists('/var/log/auth.log') else '/var/log/secure'
    for line in _read_new_log_lines(log_file, state):
        match = re.search(r'sshd.*?session opened for user (\w+)', line)
        if match:
            events.append(_create_event('USER_LOGIN', f"User '{match.group(1)}' logged in via SSH"))
            continue
        match = re.search(r'Failed password for (\w+) from ([\d.]+)', line)
        if match:
            events.append(_create_event('FAILED_LOGIN', f"Failed password for '{match.group(1)}' from {match.group(2)}"))
            continue
    return events

def parse_package_log(state):
    """Parses apt/history.log or dnf.log for package changes."""
    events = []
    log_file = None
    for f in ['/var/log/apt/history.log', '/var/log/dnf.log']:
        if os.path.exists(f):
            log_file = f
            break
    if not log_file:
        return events
    for line in _read_new_log_lines(log_file, state):
        install_match = re.search(r'Install: ([\w.+-]+):.*? \((.*?)\)', line)
        if install_match:
            pkg_name = install_match.group(1).split(':')[0]
            events.append(_create_event('PACKAGE_CHANGE', f"Package '{pkg_name}' installed (version {install_match.group(2)})"))
            continue
        upgrade_match = re.search(r'Upgrade: ([\w.+-]+):.*? \((.*?)\), ([\w.+-]+):.*? \((.*?)\)', line)
        if upgrade_match:
            pkg_name = upgrade_match.group(1).split(':')[0]
            events.append(_create_event('PACKAGE_CHANGE', f"Package '{pkg_name}' upgraded to '{upgrade_match.group(4)}'"))
            continue
        dnf_match = re.search(r'^(?:Install|Upgrade|Installed|Upgraded): ([\w-]+)-([0-9].*)', line)
        if dnf_match:
            summary = f"Package '{dnf_match.group(1)}' was installed or upgraded to version '{dnf_match.group(2)}'"
            if not any(e['summary'].startswith(f"Package '{dnf_match.group(1)}'") for e in events):
                events.append(_create_event('PACKAGE_CHANGE', summary))
    return events

def parse_syslog_and_kern(state):
    """Parses common system logs for critical error messages."""
    events = []
    log_files = {
        '/var/log/syslog': 'SYSTEM_ERROR',
        '/var/log/messages': 'SYSTEM_ERROR',
        '/var/log/kern.log': 'KERNEL_ERROR',
    }
    keywords = ['error', 'failed', 'fatal', 'segfault', 'panic', 'out of memory', 'critical']
    for log_file, event_type in log_files.items():
        for line in _read_new_log_lines(log_file, state):
            if any(keyword in line.lower() for keyword in keywords):
                summary = (line[:250] + '...') if len(line) > 253 else line
                events.append(_create_event(event_type, summary))
    return events

def get_windows_events(state):
    """Uses PowerShell to get new Windows Events (for structured event stream)."""
    events = []
    log_name = "Security"
    event_id = 4624
    state_key = f"win_event_{log_name}_{event_id}_last_id"
    last_record_id = state.get(state_key, 0)
    ps_command = f'Get-WinEvent -LogName {log_name} -FilterXPath "*[System[EventID={event_id} and EventRecordID > {last_record_id}]]" | Select-Object TimeCreated, Message, RecordId | Sort-Object RecordId | ConvertTo-Json'
    try:
        result = subprocess.run(["powershell", "-Command", ps_command], capture_output=True, text=True, check=True, timeout=20)
        win_events = json.loads(result.stdout.strip())
        if not isinstance(win_events, list):
            win_events = [win_events]
        max_id = last_record_id
        for win_event in win_events:
            summary_match = re.search(r'Account Name:\s+([\w\-$]+)', win_event['Message'])
            summary = f"User '{summary_match.group(1)}' logged on." if summary_match else "A user successfully logged on."
            events.append(_create_event('USER_LOGIN', summary))
            if win_event['RecordId'] > max_id:
                max_id = win_event['RecordId']
        state[state_key] = max_id
    except Exception as e:
        print(f"WARN: Could not execute PowerShell command for {log_name} log. Error: {e}")
    return events

def collect_events(state):
    """Gathers all system events from various sources based on the OS."""
    all_events = []
    print("--> Collecting events...")
    system = platform.system()
    if system == "Linux":
        all_events.extend(parse_auth_log(state))
        all_events.extend(parse_package_log(state))
        all_events.extend(parse_syslog_and_kern(state))
    elif system == "Windows":
        all_events.extend(get_windows_events(state))
    if all_events:
        print(f"--> Collected {len(all_events)} new events.")
    return all_events

# --- Metrics Collection ---

def collect_all_metrics():
    """Gathers all enhanced system metrics."""
    global last_net_io, last_time
    print("--> Collecting metrics...")
    psutil.cpu_percent(interval=0.1)
    cpu_percent = psutil.cpu_percent(interval=1)
    memory_percent = psutil.virtual_memory().percent
    disks = []
    for part in psutil.disk_partitions(all=False):
        if 'loop' in part.device or any(fs in part.fstype for fs in ['tmpfs', 'squashfs']):
            continue
        try:
            usage = psutil.disk_usage(part.mountpoint)
            disks.append({"device": part.device, "mountpoint": part.mountpoint, "percent": usage.percent})
        except Exception:
            continue
    procs = []
    for p in psutil.process_iter(['pid', 'name', 'username', 'cpu_percent', 'memory_percent']):
        try:
            procs.append(p.info)
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
    top_procs = sorted(procs, key=lambda p: (p.get('cpu_percent', 0) or 0, p.get('memory_percent', 0) or 0), reverse=True)
    current_net_io = psutil.net_io_counters()
    current_time = time.time()
    elapsed_time = current_time - last_time
    if elapsed_time <= 0:
        net_rate = {"bytes_sent_per_sec": 0, "bytes_recv_per_sec": 0}
    else:
        bytes_sent_rate = (current_net_io.bytes_sent - last_net_io.bytes_sent) / elapsed_time
        bytes_recv_rate = (current_net_io.bytes_recv - last_net_io.bytes_recv) / elapsed_time
        net_rate = {"bytes_sent_per_sec": int(bytes_sent_rate), "bytes_recv_per_sec": int(bytes_recv_rate)}
    last_net_io = current_net_io
    last_time = current_time
    metrics = {"cpu_percent": cpu_percent, "memory_percent": memory_percent, "disks": disks, "processes": top_procs[:10], "network": net_rate}
    print("--> Metrics collection complete.")
    return metrics

# --- Core Agent Logic ---

def load_config():
    """Loads agent configuration from the config file."""
    config = {}
    if not os.path.exists(CONFIG_FILE_PATH):
        raise FileNotFoundError(f"FATAL: Config file not found at '{CONFIG_FILE_PATH}'")
    with open(CONFIG_FILE_PATH, 'r') as f:
        for line in f:
            if '=' in line:
                key, value = line.strip().split('=', 1)
                config[key.strip()] = value.strip()
    return config

def get_service_account_credentials(key_path, target_audience):
    """Creates credentials that can be used to invoke a secured service."""
    try:
        return service_account.IDTokenCredentials.from_service_account_file(key_path, target_audience=target_audience)
    except Exception as e:
        raise Exception(f"FATAL: Could not create service account credentials. Error: {e}")

# --- Main Execution Loop ---
if __name__ == "__main__":
    print("Starting NoirNote Full State Agent...")
    try:
        config = load_config()
        credentials = get_service_account_credentials(KEY_FILE_PATH, config['INGEST_FUNCTION_URL'])
    except Exception as e:
        print(e)
        exit(1)
        
    print(f"Agent configured for server_id: {config.get('SERVER_ID', 'UNKNOWN')} "
          f"workspace_id: {config.get('WORKSPACE_ID', 'UNKNOWN')}")
    
    state = load_state()
    authed_session = google.auth.transport.requests.Request()

    while True:
        try:
            metrics = collect_all_metrics()
            events = collect_events(state)
            state_snapshot = collect_state_snapshot()  # <<< NEW FUNCTION CALL

            payload = {
                "user_id": config['USER_ID'],
                "workspace_id": config.get('WORKSPACE_ID', config['USER_ID']),
                "server_id": config['SERVER_ID'],
                "metrics": metrics,
                "events": events,
                "state": state_snapshot  # <<< NEW DATA STRUCTURE
            }
            
            credentials.refresh(authed_session)
            headers = {'Authorization': f'Bearer {credentials.token}', 'Content-Type': 'application/json'}
            
            print(f"Pushing full payload to {config['INGEST_FUNCTION_URL']}...")
            # Uncomment for deep debugging of the entire payload:
            # print(json.dumps(payload, indent=2))
            
            response = requests.post(config['INGEST_FUNCTION_URL'], json=payload, headers=headers, timeout=30)
            
            response.raise_for_status()
            print(f"Successfully pushed payload. Status: {response.status_code}")

            save_state(state)

        except requests.exceptions.RequestException as e:
            print(f"ERROR: Network error while pushing payload: {e}")
        except Exception as e:
            print(f"ERROR: An unhandled exception occurred in the main loop: {e}")
            traceback.print_exc()
        
        time.sleep(int(config.get('INTERVAL_SECONDS', 60)))

======== uninstall-agent.sh ========
#!/bin/bash

set -e # Exit immediately if a command exits with a non-zero status.

echo "--- NoirNote Agent Uninstaller ---"

# --- Configuration (Must match the installer) ---
AGENT_USER="noirnote-agent"
AGENT_DIR="/opt/noirnote-agent"
CONFIG_DIR="/etc/noirnote"
STATE_DIR="/var/lib/noirnote-agent" # <-- NEW: State directory to remove
AGENT_SERVICE_FILE="/etc/systemd/system/noirnote-agent.service"
PYTHON_PACKAGES="psutil requests google-auth"

# --- Helper Functions ---
function check_root() {
    if [ "$EUID" -ne 0 ]; then
      echo "Error: This uninstaller must be run with sudo or as root."
      exit 1
    fi
}

function stop_and_disable_service() {
    echo "--> [1/5] Stopping and disabling the systemd service..."
    if systemctl is-active --quiet noirnote-agent.service; then
        systemctl stop noirnote-agent.service
        echo "    - Service stopped."
    else
        echo "    - Service was not running."
    fi

    if systemctl is-enabled --quiet noirnote-agent.service; then
        systemctl disable noirnote-agent.service
        echo "    - Service disabled."
    else
        echo "    - Service was not enabled."
    fi
}

function remove_agent_files() {
    echo "--> [2/5] Removing agent files and directories..."
    
    # Remove the service file
    if [ -f "$AGENT_SERVICE_FILE" ]; then
        rm -f "$AGENT_SERVICE_FILE"
        echo "    - Removed systemd service file: $AGENT_SERVICE_FILE"
        # Tell systemd to re-read its configuration
        systemctl daemon-reload
        echo "    - Reloaded systemd daemon."
    else
        echo "    - Service file not found (already removed)."
    fi

    # Remove the agent's main directory
    if [ -d "$AGENT_DIR" ]; then
        rm -rf "$AGENT_DIR"
        echo "    - Removed agent directory: $AGENT_DIR"
    else
        echo "    - Agent directory not found (already removed)."
    fi

    # Remove the agent's configuration directory
    if [ -d "$CONFIG_DIR" ]; then
        rm -rf "$CONFIG_DIR"
        echo "    - Removed configuration directory: $CONFIG_DIR"
    else
        echo "    - Configuration directory not found (already removed)."
    fi

    # Remove the agent's state directory
    if [ -d "$STATE_DIR" ]; then
        rm -rf "$STATE_DIR"
        echo "    - Removed agent state directory: $STATE_DIR"
    else
        echo "    - Agent state directory not found (already removed)."
    fi
}

function remove_agent_user() {
    echo "--> [3/5] Removing agent user..."
    if id -u "$AGENT_USER" >/dev/null 2>&1; then
        # The `userdel` command automatically removes the user from any groups.
        userdel "$AGENT_USER"
        echo "    - Removed system user '$AGENT_USER'."
    else
        echo "    - System user '$AGENT_USER' not found (already removed)."
    fi
}

function uninstall_python_deps() {
    echo "--> [4/5] Uninstalling Python dependencies..."
    if command -v pip3 &> /dev/null; then
        # The -y flag confirms the uninstallation without a prompt
        pip3 uninstall -y $PYTHON_PACKAGES > /dev/null 2>&1
        echo "    - Uninstalled Python packages: $PYTHON_PACKAGES"
    else
        echo "    - pip3 not found, skipping Python package uninstallation."
    fi
    echo "    - Note: System-level packages like python3, curl, etc., are not removed."
}

function final_summary() {
    echo ""
    echo "--- Uninstallation Complete! ---"
    echo "The NoirNote agent and all its components have been removed."
    echo "You may want to run 'sudo apt-get autoremove' to clean up any other unused dependencies."
}

# --- Main Execution ---
check_root
stop_and_disable_service
remove_agent_files
remove_agent_user
uninstall_python_deps
final_summary

